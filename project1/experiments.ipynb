{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 2024 - project 1\n",
    "Kinga FraÅ„czak, 313335\n",
    "\n",
    "Grzegorz Zakrzewski, 313555"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow==2.10.1\n",
    "# pandas==1.5.3\n",
    "# numpy==1.26.4\n",
    "# seaborn==0.13.2\n",
    "# matplotlib==3.8.3\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=0.9)\n",
    "np.random.seed(0)\n",
    "keras.utils.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPETITIONS = 3  # number of repetitions of each experiment\n",
    "EPOCHS = 30  # number of epochs usually used to train the models\n",
    "\n",
    "directory_train = './archive/cinic-10_image_classification_challenge-dataset/train'\n",
    "directory_test = './archive/cinic-10_image_classification_challenge-dataset/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory_train,\n",
    "    seed=0,\n",
    "    image_size=(32, 32),\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(\n",
    "    model,\n",
    "    name=\"\",\n",
    "    callbacks=[],\n",
    "    optimizer=\"sgd\",\n",
    "    epochs=EPOCHS,\n",
    "    repetitions=REPETITIONS,\n",
    "):\n",
    "    results = []\n",
    "    for i in range(repetitions):\n",
    "        model = keras.models.clone_model(model)\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "        history = model.fit(ds_train, validation_data=ds_valid, epochs=epochs, callbacks=callbacks)\n",
    "        frame = pd.DataFrame(history.history)\n",
    "        frame[\"Name\"] = name\n",
    "        frame[\"Repetition\"] = i\n",
    "        frame = frame.reset_index().rename(columns={\"index\": \"Epoch\"})\n",
    "        results.append(frame)\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 0.1 - simple convolutional neural network - testing various architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model I - one convolutional layer \n",
    "\n",
    "model1 = keras.models.Sequential()\n",
    "\n",
    "model1.add(keras.Input(shape=(32, 32, 3)))\n",
    "\n",
    "# Convolutional layers\n",
    "model1.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model1.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(keras.layers.BatchNormalization())\n",
    "model1.add(keras.layers.Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model1.add(keras.layers.Dense(32, activation='relu')) \n",
    "model1.add(keras.layers.Dense(10, activation='softmax')) \n",
    "\n",
    "results1 = fit_model(model1, name=\"1 conv layer\")\n",
    "results1.to_csv(\"results/experiment01_results1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model II - two convolutional layers \n",
    "\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(keras.Input(shape=(32, 32, 3)))\n",
    "model2.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model2.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model2.add(keras.layers.BatchNormalization())\n",
    "model2.add(keras.layers.Flatten())\n",
    "model2.add(keras.layers.Dense(32, activation='relu')) \n",
    "model2.add(keras.layers.Dense(10, activation='softmax')) \n",
    "\n",
    "results2 = fit_model(model2, name=\"2 conv layers\")\n",
    "results2.to_csv(\"results/experiment01_results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model III - three convolutional layers \n",
    "\n",
    "model3 = keras.models.Sequential()\n",
    "model3.add(keras.Input(shape=(32, 32, 3)))\n",
    "model3.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model3.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model3.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model3.add(keras.layers.BatchNormalization())\n",
    "model3.add(keras.layers.Flatten())\n",
    "model3.add(keras.layers.Dense(32, activation='relu')) \n",
    "model3.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "results3 = fit_model(model3, name=\"3 conv layers\")\n",
    "results3.to_csv(\"results/experiment01_results3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pd.read_csv(\"results/experiment01_results1.csv\")\n",
    "results2 = pd.read_csv(\"results/experiment01_results2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment01_results = pd.concat([results1, results2, results3])\n",
    "experiment01_results.to_csv(\"results/experiment01_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1 - Testing hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1a - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model -> the best model from Experiment 0.1\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.Input(shape=(32, 32, 3)))\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(32, activation='relu')) \n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.1. - optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZERS = [\"sgd\", \"rmsprop\", \"adam\", \"adadelta\"]\n",
    "experiment11_results = []\n",
    "\n",
    "for optimizer in OPTIMIZERS:\n",
    "    results = fit_model(model, name=optimizer, optimizer=optimizer)\n",
    "    experiment11_results.append(results)\n",
    "\n",
    "experiment11_results = pd.concat(experiment11_results)\n",
    "experiment11_results.to_csv(\"results/experiment11_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.2. - learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best optimazer from Experiment 1.1 is: \n",
    "    # Adam optimizer\n",
    "\n",
    "experiment12_results = []\n",
    "\n",
    "for learning_rate in [0.0001, 0.0005, 0.001, 0.005, 0.01]:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    results = fit_model(model, name=f\"Learning rate = {learning_rate}\", optimizer=optimizer)\n",
    "    experiment12_results.append(results)\n",
    "\n",
    "experiment12_results = pd.concat(experiment12_results)\n",
    "experiment12_results.to_csv(\"results/experiment12_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1b - regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the best optimizer and with the finest value of learning rate parameter from Experiments 1.1 and 1.2\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.3. - dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment13_results = []\n",
    "\n",
    "for dropout_rate in [0.2, 0.4, 0.6]:\n",
    "\n",
    "    # the best model from Experiment 0.1\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.Input(shape=(32, 32, 3)))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    # dropout\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu')) \n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    results = fit_model(model, name=f\"Dropout rate = {dropout_rate}\", optimizer=optimizer)\n",
    "    experiment13_results.append(results)\n",
    "\n",
    "experiment13_results = pd.concat(experiment13_results)\n",
    "experiment13_results.to_csv(\"results/experiment13_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1.4. - early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model with best architecture, optimizer, learning rate and dropout from previous experiments\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.Input(shape=(32, 32, 3)))\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(32, activation='relu')) \n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment14_results = []\n",
    "\n",
    "for patience in [2, 4, 6]:\n",
    "    callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience)\n",
    "\n",
    "    results = fit_model(model, name=f\"Patience = {patience}\", optimizer=optimizer, callbacks=[callback], epochs=40)\n",
    "    experiment14_results.append(results)\n",
    "\n",
    "experiment14_results = pd.concat(experiment14_results)\n",
    "experiment14_results.to_csv(\"results/experiment14_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 - Testing augmentation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1 - image flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.RandomFlip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.2 - image rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.RandomRotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.3 - image shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.RandomTranslation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.4 - MixUp augmentation (more advanced technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/vision/mixup/\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "from tensorflow.random import gamma as tf_random_gamma\n",
    "\n",
    "AUTO = tf_data.AUTOTUNE\n",
    "\n",
    "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
    "    gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)\n",
    "    gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)\n",
    "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
    "\n",
    "\n",
    "def mix_up(ds_one, ds_two, alpha=0.2):\n",
    "    # Unpack two datasets\n",
    "    images_one, labels_one = ds_one\n",
    "    images_two, labels_two = ds_two\n",
    "    batch_size = keras.ops.shape(images_one)[0]\n",
    "\n",
    "    # Sample lambda and reshape it to do the mixup\n",
    "    l = sample_beta_distribution(batch_size, alpha, alpha)\n",
    "    x_l = keras.ops.reshape(l, (batch_size, 1, 1, 1))\n",
    "    y_l = keras.ops.reshape(l, (batch_size, 1))\n",
    "\n",
    "    # Perform mixup on both images and labels by combining a pair of images/labels\n",
    "    # (one from each dataset) into one image/label\n",
    "    images = images_one * x_l + images_two * (1 - x_l)\n",
    "    labels = labels_one * y_l + labels_two * (1 - y_l)\n",
    "    return (images, labels)\n",
    "\n",
    "ds_train_mixup = ds_train.map(\n",
    "    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2),\n",
    "    num_parallel_calls=AUTO,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 - Testing pre-trained models "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
